{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  question 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "def sigmoid_deriv(X):\n",
    "    return sigmoid(X)*(1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  The function above is the sigmoid function used for activation. this activation function adds non-linearity to the output of a neuron\n",
    "\n",
    "# sigmoid_derivative is the calculation of sigmoid when doing back propagation(derivative of zigmoid with respect to that particular neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, w1, w2, predict = False):\n",
    "    a1 = np.matmul(X, w1)\n",
    "    z1 = sigmoid(a1)\n",
    "    \n",
    "    #then create and add bias\n",
    "    bias = np.ones((len(z1), 1))\n",
    "    z1 = np.concatenate((bias, z1), axis = 1)\n",
    "    a2 = np.matmul(z1, w2)\n",
    "    z2 = sigmoid(a2)\n",
    "    if predict:\n",
    "        return z2\n",
    "    return a1, z1, a2, z2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the function that calculates the farward propagation. in this function, the bias is added to the inner layer. the inner bias is involved in thecalculation of the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(a1, z0, z1 , z2, y):\n",
    "    delta2 = (z2 - y)\n",
    "    Delta2 = np.matmul(z1.T, delta2)\n",
    "    delta1 = (delta2.dot(w2[1:,:].T))*sigmoid_deriv(a1)\n",
    "    Delta1 = np.matmul(z0.T, delta1)\n",
    "    return delta2, Delta1, Delta2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The backprop function calculates the derivative of the loss with respects to all the weights in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1, 0], # This is the dataset, i inserted ths bias column.\n",
    "              [1, 0, 1],\n",
    "              [1, 0, 0],\n",
    "              [1, 1, 1]])\n",
    "y = np.array([[1], [1], [0], [0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.randn(3,3)\n",
    "w2 = np.random.randn(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0. Error: 0.500406770923803\n",
      "iteration : 500. Error: 0.49565554436013837\n",
      "iteration : 1000. Error: 0.47608217913130374\n",
      "iteration : 1500. Error: 0.394482609159217\n",
      "iteration : 2000. Error: 0.2444258279510303\n",
      "iteration : 2500. Error: 0.14120732421400642\n",
      "iteration : 3000. Error: 0.0915151229828831\n",
      "iteration : 3500. Error: 0.0657250292875743\n",
      "iteration : 4000. Error: 0.05061448677091135\n",
      "iteration : 4500. Error: 0.040878910014425625\n",
      "training complete\n",
      "percentages:\n",
      "[[0.97081715]\n",
      " [0.95459033]\n",
      " [0.03394683]\n",
      " [0.02806889]]\n",
      " predictions:  with stochastic gradient descent\n",
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "epochs = 5000\n",
    "m = len(X)\n",
    "costs = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    #forward propagation\n",
    "    a1, z1, a2, z2 = forward(X, w1, w2)\n",
    "    \n",
    "    #back propagation\n",
    "    delta2, Delta1, Delta2 = backprop(a1, X, z1, z2, y)\n",
    "    w1 -= lr * (1/m) * Delta1\n",
    "    w2 -= lr * (1/m) * Delta2\n",
    "    \n",
    "    c = np.mean(np.abs(delta2))\n",
    "    costs.append(c)\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print(f\"iteration : {i}. Error: {c}\")\n",
    "print(\"training complete\")\n",
    "\n",
    "\n",
    "z3 = forward(X, w1, w2, True)\n",
    "print (\"percentages:\")\n",
    "print(z3)\n",
    "print(\" predictions:  with stochastic gradient descent\")\n",
    "print(np.round(z3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
